{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961f54c1-8eae-4ffa-af08-584e8612d198",
   "metadata": {},
   "source": [
    "# experimental\n",
    "> Various tools that I'm not so sure about that are at the bleeding edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c02c61-7fe1-4dbc-b74c-bf11a64ca9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd47cc-3f52-4cfc-9efb-4dc6a6c61993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from typing import List, Iterable, Union\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "import langsmith\n",
    "from fastcore.foundation import first, L\n",
    "from fastcore.test import test_eq\n",
    "from langfree.runs import (get_runs, get_output, get_input, \n",
    "                           get_params, get_functions,\n",
    "                          get_feedback, client, take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7012b46-4c1b-442f-9d11-d6e2fc69c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LLMRecord(BaseModel):\n",
    "    \"A parsed run from LangSmith.\"\n",
    "    child_run_id:str\n",
    "    parent_run_id:str\n",
    "    llm_input:str\n",
    "    llm_output: str\n",
    "    url: str\n",
    "    total_tokens:Union[int, None]\n",
    "    prompt_tokens:Union[int, None]\n",
    "    completion_tokens:Union[int, None]\n",
    "    human_readable:bool=False\n",
    "    feedback: Union[List,None] = None\n",
    "    feedback_keys: Union[List,None] = None\n",
    "    tags: Union[List,None] = []\n",
    "    start_dt: Union[str, None] = None\n",
    "    parent_url: Union[str,None] = None\n",
    "    parent_id: Union[str,None] = None\n",
    "    function_defs: Union[List,None] = None\n",
    "    param_model_name: Union[str,None]= None\n",
    "    param_n: Union[int, None] = None\n",
    "    param_top_p: Union[int, None] = None\n",
    "    param_temp: Union[int, None] = None\n",
    "    param_presence_penalty: Union[int, None] = None\n",
    "    param_freq_penalty: Union[int, None] = None\n",
    "    error_categories: List[str] = []\n",
    "    \n",
    "    @classmethod\n",
    "    def collate(cls, run:langsmith.schemas.Run, human_readable=False):\n",
    "        \"Collate information About A Run into a `LLMRecord`.\"\n",
    "        error_categories = []\n",
    "        if run.execution_order != 1: # this is a child run, get the parent\n",
    "            crun = run\n",
    "            run = client.read_run(run.parent_run_id)\n",
    "            \n",
    "        _cruns = client.read_run(run_id=run.id, load_child_runs=True).child_runs\n",
    "        crun = _cruns[-1] if _cruns else None\n",
    "\n",
    "    \n",
    "        if crun:\n",
    "            if crun.name != 'ChatOpenAI': \n",
    "                error_categories.append('Last Step Not ChatOpenAI')\n",
    "                _input, _output = '', ''\n",
    "            else: _input, _output = get_input(crun), get_output(crun)\n",
    "                \n",
    "            if 'Agent stopped due to max iterations' in _input: error_categories.append('Max Iterations')\n",
    "            if _output.strip() == '': error_categories.append('No Output')\n",
    "            \n",
    "            params = get_params(crun)\n",
    "            _feedback = get_feedback(run) # you must get feedback from the root\n",
    "            \n",
    "            return cls(child_run_id=str(crun.id),\n",
    "                       parent_run_id=str(run.id),\n",
    "                       llm_input=_input,\n",
    "                       llm_output=_output,\n",
    "                       url=crun.url,\n",
    "                       total_tokens=crun.total_tokens,\n",
    "                       prompt_tokens=crun.prompt_tokens,\n",
    "                       completion_tokens=crun.completion_tokens,\n",
    "                       human_readable=human_readable,\n",
    "                       feedback=_feedback, \n",
    "                       feedback_keys=list(L(_feedback).attrgot('key').filter()),\n",
    "                       tags=run.tags,\n",
    "                       start_dt=run.start_time.strftime('%m/%d/%Y'),\n",
    "                       parent_url=run.url if run else None,\n",
    "                       parent_id=str(run.id) if run else None,\n",
    "                       function_defs=get_functions(crun),\n",
    "                       error_categories=error_categories,\n",
    "                       **params)\n",
    "        \n",
    "    def _repr_markdown_(self):\n",
    "        if not self.human_readable: return None\n",
    "        feedback_str = ''\n",
    "        if self.feedback:\n",
    "            for f in self.feedback:\n",
    "                for k,v in f.items():\n",
    "                    feedback_str+=f'{k}: {v}\\n'\n",
    "        \n",
    "        raw_markdown =  f\"\"\"\n",
    "<details>\n",
    "  <summary>Show/Collapse Run</summary>\n",
    "  \n",
    "# URLs\n",
    "Child: {self.url}\n",
    "\n",
    "Parent: {self.parent_url}\n",
    "\n",
    "Tags: {self.tags}\n",
    "\n",
    "# Inputs:\n",
    "\n",
    "{self.llm_input}\n",
    "\n",
    "# Outputs:\n",
    "\n",
    "{self.llm_output}\n",
    "\n",
    "# Feedback: {bool(self.feedback)}\n",
    "\n",
    "{feedback_str}\n",
    "\n",
    "# Error Categories: {bool(self.error_categories)}\n",
    "\n",
    "{self.error_categories}\n",
    "</details>\n",
    "\"\"\"\n",
    "        blockquoted_markdown = '\\n'.join([f'> {line}' for line in raw_markdown.split('\\n')])\n",
    "        return blockquoted_markdown\n",
    "    \n",
    "    def show(self):\n",
    "        self.human_readable=True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd718377-5738-4179-9708-0bcf0d0108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "_root_run = client.read_run('fbfd220a-c731-46a2-87b3-e64a477824f5')\n",
    "\n",
    "_child = client.read_run(_root_run.child_run_ids[-1])\n",
    "_root_result = LLMRecord.collate(_root_run)\n",
    "_child_result = LLMRecord.collate(_child)\n",
    "\n",
    "# test that child and root runs are related\n",
    "test_eq(_root_result.llm_output, _child_result.llm_output)\n",
    "test_eq(_root_result.parent_run_id, _child_result.parent_run_id)\n",
    "\n",
    "# Test case without feedback\n",
    "_parent_run_no_feedback = client.read_run('87900cfc-0322-48fb-b009-33d226d73597')\n",
    "_no_feedback = LLMRecord.collate(_parent_run_no_feedback)\n",
    "test_eq(_no_feedback.feedback, [])\n",
    "\n",
    "# Test case with feedback\n",
    "\n",
    "#  ... starting with a child run\n",
    "_child_w_feedback = client.read_run('f8717b0e-fb90-45cd-be00-9b4614965a2e')\n",
    "_feedback = LLMRecord.collate(_child_w_feedback).feedback\n",
    "assert _feedback[0]['key'] == 'Empty Response'\n",
    "\n",
    "# #  ... starting with a parent run\n",
    "_parent_w_feedback = client.read_run(_child_w_feedback.parent_run_id)\n",
    "_feedback2 = LLMRecord.collate(_parent_w_feedback).feedback\n",
    "test_eq(_feedback[0]['comment'],  _feedback2[0]['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b0e08-f640-441c-b825-da25f0f747ff",
   "metadata": {},
   "source": [
    "## Saving & Loading A Dataset of `LLMRecord`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7563fc-b096-4f4f-ad6a-2fcf00485d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LLMDataset(BaseModel):\n",
    "    \"A collection of `LLMRecord`.\"\n",
    "    records: List[LLMRecord]\n",
    "    tags: Counter\n",
    "    dates: Counter\n",
    "    \n",
    "    @classmethod\n",
    "    def from_commit(cls, commit_id:str):\n",
    "        \"Create a `LLMDataset` from a commit id\"\n",
    "        _runs = get_runs(commit_id=commit_id)\n",
    "        return cls.from_runs(_runs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_runs(cls, runs:List[langsmith.schemas.Run]):\n",
    "        \"Load LLMDataset from runs.\"\n",
    "        tag_counter=Counter()\n",
    "        date_counter=Counter()\n",
    "        records=[LLMRecord.collate(r, human_readable=False) for r in runs]\n",
    "        for rec in records:\n",
    "            if rec.tags: tag_counter.update(rec.tags)\n",
    "            if rec.start_dt: date_counter.update([rec.start_dt])\n",
    "        return cls(records=records, tags=tag_counter, dates=date_counter)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    \n",
    "    def save(self, path:str):\n",
    "        \"Save data to disk.\"\n",
    "        dest_path = Path(path)\n",
    "        if not dest_path.parent.exists(): dest_path.parent.mkdir(exist_ok=True)\n",
    "        with open(dest_path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "            return dest_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for r in self.records: yield r\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path:str):\n",
    "        \"Load data from disk.\"\n",
    "        src_path = Path(path)\n",
    "        with open(src_path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "            if isinstance(obj, cls):\n",
    "                return obj\n",
    "            else:\n",
    "                raise TypeError(f\"The loaded object is not of type {cls.__name__}\")\n",
    "                \n",
    "    def to_pandas(self):\n",
    "        \"Convert the `LLMDataset` to a pandas.DataFrame.\"\n",
    "        return pd.DataFrame(L(self.records).map(dict))\n",
    "    \n",
    "    def to_airtable_csv(self, path:str):\n",
    "        \"Format a csv such that it can be exported to Airtable conveniently.\"\n",
    "        dest_path = Path(path)\n",
    "        if not dest_path.parent.exists(): dest_path.parent.mkdir(exist_ok=True)\n",
    "        if dest_path.suffix != '.csv': raise Exception(f\"You must name your file with a csv extension, instead got {path}\")\n",
    "        data = deepcopy(self)\n",
    "        for r in data.records:\n",
    "            r.tags = ', '.join(r.tags)\n",
    "            r.feedback_keys = ', '.join(r.feedback_keys)\n",
    "            r.error_categories = ', '.join(r.error_categories)\n",
    "        df = pd.DataFrame(L(data.records).map(dict))\n",
    "        df.to_csv(dest_path)\n",
    "        return dest_path\n",
    "                \n",
    "    def __repr__(self):\n",
    "        tags = '\\n'.join([f'- {x} {y}' for x,y in self.tags.most_common(5)])\n",
    "        dates = '\\n'.join([f'- {x} {y}' for x,y in self.dates.most_common(5)])\n",
    "        return f'LLMDataset with {len(self)} records.\\n\\nDates:\\n{dates}\\n\\n5 Most common tags:\\n{tags}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f451f5-ab9a-4e0a-b338-912d941dd5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching runs with this filter: and(eq(status, \"success\"), has(tags, \"commit:028e4aa4\"))\n"
     ]
    }
   ],
   "source": [
    "from langfree.runs import get_runs\n",
    "_runs = get_runs(commit_id='028e4aa4')\n",
    "llmdata = LLMDataset.from_runs(take(_runs, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d487386-c66d-44ea-b7e9-8dba650aa917",
   "metadata": {},
   "source": [
    "### Convert `LLMDataset` to a Pandas Dataframe\n",
    "\n",
    "You can do this with `to_pandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69f78a-443b-4d1d-80d2-6f2b9754e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = llmdata.to_pandas()\n",
    "assert _df.shape[0] == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b1760-faa5-42bd-97d9-cb3e4348ed3f",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c2214-a08e-4cc4-9188-ec755d1d8b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('_data/llm_data.pkl')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "llmdata.save('_data/llm_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dcb976-bb6c-4909-b8b0-0e8099272e81",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689b521-7f85-406f-9122-618dcae57d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "_loaded = LLMDataset.load('_data/llm_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9def71-443b-42c3-8527-a08e847d251d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
